# app.py ‚Äî Enhanced Flask with Multi-Algorithm OCHL Prediction + ALL Features + Detailed Output
import os
import time
import random
import json
import threading
import warnings
import gc
from functools import wraps
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed

import pandas as pd
import numpy as np
import requests
import yfinance as yf
from flask import Flask, send_from_directory, render_template, jsonify, request, redirect

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.ensemble import IsolationForest
from sklearn.neural_network import MLPRegressor
import joblib
import pmdarima as pm
from statsmodels.tsa.arima.model import ARIMA as StatsmodelsARIMA
warnings.filterwarnings('ignore')

# ---------------- Config ----------------
MODELS_DIR = 'models'
HISTORY_DIR = 'history'
CACHE_DIR = 'cache'
os.makedirs(MODELS_DIR, exist_ok=True)
os.makedirs(HISTORY_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

# ---------------- Flask ----------------
current_dir = os.path.dirname(os.path.abspath(__file__))
server = Flask(__name__, template_folder='templates', static_folder='static')

# ---------------- Rate limiter ----------------
class RateLimiter:
    def __init__(self, max_per_minute=30):
        self.max_per_minute = max_per_minute
        self.min_interval = 60.0 / max_per_minute
        self.last_called = {}
        self.lock = threading.Lock()
    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            func_name = func.__name__
            with self.lock:
                current_time = time.time()
                last_time = self.last_called.get(func_name, 0)
                elapsed = current_time - last_time
                if elapsed < self.min_interval:
                    sleep_time = self.min_interval - elapsed
                    time.sleep(sleep_time)
                self.last_called[func_name] = time.time()
            return func(*args, **kwargs)
        return wrapper

rate_limiter = RateLimiter(max_per_minute=25)

# ---------------- CRITICAL FIXES: Stock Split Handling ----------------
KNOWN_STOCK_SPLITS = {
    'GE': {'date': '2021-07-30', 'ratio': 8, 'type': 'reverse'},  # 1-for-8 reverse split
    'AAPL': {'date': '2020-08-31', 'ratio': 4, 'type': 'forward'},  # 4-for-1 split
    'TSLA': {'date': '2022-08-25', 'ratio': 3, 'type': 'forward'},  # 3-for-1 split
    'NVDA': {'date': '2021-07-20', 'ratio': 4, 'type': 'forward'},  # 4-for-1 split
    'GOOGL': {'date': '2022-07-18', 'ratio': 20, 'type': 'forward'},  # 20-for-1 split
    'AMZN': {'date': '2022-06-06', 'ratio': 20, 'type': 'forward'},  # 20-for-1 split
}

def detect_and_handle_splits(data, ticker):
    """Detect stock splits and return clean, post-split only data"""
    try:
        ticker_upper = ticker.upper()
        
        # Check if we know about this stock's split
        if ticker_upper in KNOWN_STOCK_SPLITS:
            split_info = KNOWN_STOCK_SPLITS[ticker_upper]
            split_date = pd.to_datetime(split_info['date'])
            split_ratio = split_info['ratio']
            split_type = split_info['type']
            
            print(f"üìä {ticker} has known {split_type} split on {split_date.date()} (1-for-{split_ratio})")
            
            # Ensure we have Date column
            if 'Date' in data.columns:
                data['Date'] = pd.to_datetime(data['Date'])
                
                # Separate pre-split and post-split data
                pre_split_data = data[data['Date'] <= split_date]
                post_split_data = data[data['Date'] > split_date]
                
                print(f"   Pre-split data: {len(pre_split_data)} days")
                print(f"   Post-split data: {len(post_split_data)} days")
                
                # Use ONLY post-split data for training/prediction
                if len(post_split_data) > 100:
                    print(f"‚úÖ Using ONLY post-split data ({len(post_split_data)} days)")
                    return post_split_data, True, split_info
                else:
                    print(f"‚ö†Ô∏è Insufficient post-split data, using all data")
                    return data, False, None
            else:
                print(f"‚ö†Ô∏è No Date column in data")
                return data, False, None
        
        # Auto-detect splits by looking for extreme price changes
        if 'Close' in data.columns and len(data) > 100:
            # Calculate daily returns
            daily_returns = data['Close'].pct_change().abs()
            
            # Look for >100% or >-50% single-day changes (indicative of splits)
            potential_splits = daily_returns[daily_returns > 1.0]  # >100% change
            
            if len(potential_splits) > 0:
                print(f"‚ö†Ô∏è Auto-detected potential split(s) in {ticker}")
                for date_idx in potential_splits.index[:3]:  # Show first 3
                    if 'Date' in data.columns and date_idx < len(data):
                        split_date = data['Date'].iloc[date_idx] if isinstance(data['Date'].iloc[date_idx], str) else data['Date'].iloc[date_idx]
                        price_before = data['Close'].iloc[date_idx-1] if date_idx > 0 else 0
                        price_after = data['Close'].iloc[date_idx] if date_idx < len(data) else 0
                        if price_before > 0:
                            ratio = price_after / price_before
                            print(f"   Possible split around {split_date}: {price_before:.2f} ‚Üí {price_after:.2f} (ratio: {ratio:.3f})")
                
                # Use data after the last detected potential split
                last_split_idx = potential_splits.index[-1]
                post_split_data = data.iloc[last_split_idx+1:].copy() if last_split_idx+1 < len(data) else data
                
                if len(post_split_data) > 100:
                    print(f"‚úÖ Using data after last potential split ({len(post_split_data)} days)")
                    return post_split_data, True, {'date': str(potential_splits.index[-1]), 'ratio': 1.0, 'type': 'auto-detected'}
        
        return data, False, None
        
    except Exception as e:
        print(f"Error in split detection: {e}")
        return data, False, None

def sanity_check_prediction(predicted_price, current_price, algo_name, max_daily_change=0.20):
    """
    CRITICAL: Sanity check for predictions
    Stocks rarely move >20% in a day without news
    """
    if predicted_price is None or np.isnan(predicted_price) or np.isinf(predicted_price):
        return False, 0, f"{algo_name}: Invalid value"
    
    if predicted_price <= 0:
        return False, 0, f"{algo_name}: Negative/zero price"
    
    if current_price <= 0:
        return False, 0, f"{algo_name}: Invalid current price"
    
    # Calculate percentage change
    pct_change = abs(predicted_price - current_price) / current_price
    
    # REJECT IMPOSSIBLE PREDICTIONS
    if pct_change > max_daily_change:  # >20% daily change is extremely rare
        return False, 0, f"{algo_name}: Implausible {pct_change*100:.1f}% daily change"
    
    if predicted_price > current_price * 3:  # >3x current price tomorrow
        return False, 0, f"{algo_name}: Impossibly high (>{current_price*3:.0f})"
    
    if predicted_price < current_price * 0.3:  # <30% of current price tomorrow
        return False, 0, f"{algo_name}: Impossibly low (<{current_price*0.3:.0f})"
    
    # Apply confidence penalty for large (but possible) changes
    confidence_penalty = 0
    if pct_change > 0.15:  # >15% change
        confidence_penalty = 40  # Heavily penalize
    elif pct_change > 0.10:  # >10% change
        confidence_penalty = 25
    elif pct_change > 0.05:  # >5% change
        confidence_penalty = 10
    
    return True, confidence_penalty, f"{algo_name}: Valid (change: {pct_change*100:.1f}%)"

# ---------------- Utility & feature functions ----------------
def calculate_rsi(prices, window=14):
    """Calculate RSI with NaN protection"""
    try:
        if len(prices) < window:
            return pd.Series([50] * len(prices), index=prices.index)
        
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).fillna(0)
        loss = (-delta.where(delta < 0, 0)).fillna(0)
        
        avg_gain = gain.rolling(window=window, min_periods=1).mean()
        avg_loss = loss.rolling(window=window, min_periods=1).mean()
        
        rs = avg_gain / avg_loss.replace(0, 1e-10)
        rsi = 100 - (100 / (1 + rs))
        return rsi.fillna(50)
    except:
        return pd.Series([50] * len(prices), index=prices.index)

def calculate_bollinger_bands(data, window=20):
    """Calculate Bollinger Bands with NaN protection"""
    try:
        if len(data) < window:
            sma = data['Close']
            std = pd.Series([0] * len(data), index=data.index)
        else:
            sma = data['Close'].rolling(window=window, min_periods=1).mean()
            std = data['Close'].rolling(window=window, min_periods=1).std().fillna(0)
        
        upper_band = sma + (std * 2)
        lower_band = sma - (std * 2)
        return sma, upper_band, lower_band
    except:
        return data['Close'], data['Close'] * 1.1, data['Close'] * 0.9

def calculate_macd(data, fast=12, slow=26, signal=9):
    """Calculate MACD with NaN protection"""
    try:
        exp1 = data['Close'].ewm(span=fast, adjust=False).mean()
        exp2 = data['Close'].ewm(span=slow, adjust=False).mean()
        macd = exp1 - exp2
        signal_line = macd.ewm(span=signal, adjust=False).mean()
        histogram = macd - signal_line
        return macd, signal_line, histogram
    except:
        return pd.Series(0, index=data.index), pd.Series(0, index=data.index), pd.Series(0, index=data.index)

def safe_divide(a, b, default=1.0):
    """Safe division with default value"""
    try:
        result = np.divide(a, b, out=np.full_like(a, default, dtype=float), where=b!=0)
        return result
    except:
        return np.full_like(a, default, dtype=float)

def create_advanced_features(data):
    """Create comprehensive features for OCHL prediction with ALL features"""
    try:
        # Create a copy to avoid modifying original
        data = data.copy()
        
        # Ensure we have the required columns
        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in required_cols:
            if col not in data.columns:
                print(f"Warning: Missing column {col} in data")
                if col == 'Volume':
                    data[col] = 1000000
                else:
                    data[col] = 100.0
        
        # Convert to numeric and handle NaNs
        for col in data.columns:
            if col != 'Date':
                data[col] = pd.to_numeric(data[col], errors='coerce')
                # Forward fill, then backward fill
                data[col] = data[col].ffill().bfill()
                # If still NaN, fill with appropriate values
                if data[col].isna().any():
                    if col == 'Volume':
                        data[col] = data[col].fillna(1000000)
                    elif col in ['Open', 'High', 'Low', 'Close']:
                        data[col] = data[col].fillna(100.0)
                    else:
                        data[col] = data[col].fillna(0)
        
        # 1. BASIC PRICE FEATURES
        data['Return'] = data['Close'].pct_change().fillna(0)
        data['Log_Return'] = np.log(data['Close'] / data['Close'].shift(1).replace(0, 1e-10)).fillna(0)
        data['Volatility_5d'] = data['Return'].rolling(window=5, min_periods=1).std().fillna(0)
        data['Volatility_20d'] = data['Return'].rolling(window=20, min_periods=1).std().fillna(0)
        
        # Price ranges
        data['High_Low_Range'] = safe_divide(data['High'] - data['Low'], data['Close'].replace(0, 1e-10), 0.02)
        data['Open_Close_Range'] = safe_divide(data['Close'] - data['Open'], data['Open'].replace(0, 1e-10), 0)
        
        # 2. MOVING AVERAGES
        for window in [5, 10, 20, 50]:
            data[f'MA_{window}'] = data['Close'].rolling(window=window, min_periods=1).mean().fillna(data['Close'])
            data[f'MA_Ratio_{window}'] = safe_divide(data['Close'], data[f'MA_{window}'].replace(0, 1e-10), 1.0)
        
        # Moving average crossovers
        data['MA5_Above_MA20'] = (data['MA_5'] > data['MA_20']).astype(int)
        data['Golden_Cross'] = ((data['MA_5'] > data['MA_20']) & 
                               (data['MA_5'].shift(1) <= data['MA_20'].shift(1))).astype(int)
        
        # 3. VOLUME FEATURES
        data['Volume_MA_10'] = data['Volume'].rolling(window=10, min_periods=1).mean().fillna(data['Volume'])
        data['Volume_Ratio'] = safe_divide(data['Volume'], data['Volume_MA_10'].replace(0, 1e-10), 1.0)
        data['Volume_Change'] = data['Volume'].pct_change().fillna(0)
        
        # Volume-Price Trend
        data['Volume_Price_Trend'] = data['Volume'] * data['Return']
        
        # On-Balance Volume (OBV)
        data['OBV'] = (np.sign(data['Return']) * data['Volume']).fillna(0).cumsum()
        
        # 4. TECHNICAL INDICATORS
        # RSI
        data['RSI_14'] = calculate_rsi(data['Close'], 14)
        data['RSI_7'] = calculate_rsi(data['Close'], 7)
        
        # RSI signals
        data['RSI_Overbought'] = (data['RSI_14'] > 70).astype(int)
        data['RSI_Oversold'] = (data['RSI_14'] < 30).astype(int)
        
        # MACD
        macd, macd_signal, macd_hist = calculate_macd(data)
        data['MACD'] = macd
        data['MACD_Signal'] = macd_signal
        data['MACD_Histogram'] = macd_hist
        data['MACD_Signal_Cross'] = (data['MACD'] > data['MACD_Signal']).astype(int)
        
        # Bollinger Bands
        bb_middle, bb_upper, bb_lower = calculate_bollinger_bands(data)
        data['BB_Middle'] = bb_middle
        data['BB_Upper'] = bb_upper
        data['BB_Lower'] = bb_lower
        data['BB_Width'] = safe_divide(bb_upper - bb_lower, bb_middle.replace(0, 1e-10), 0.1)
        data['BB_Position'] = safe_divide(data['Close'] - bb_lower, (bb_upper - bb_lower).replace(0, 1e-10), 0.5)
        data['BB_Upper_Touch'] = (data['Close'] >= data['BB_Upper'] * 0.99).astype(int)
        data['BB_Lower_Touch'] = (data['Close'] <= data['BB_Lower'] * 1.01).astype(int)
        
        # 5. MOMENTUM & OSCILLATORS
        # Momentum
        for window in [5, 10, 20]:
            shifted = data['Close'].shift(window).replace(0, 1e-10)
            data[f'Momentum_{window}'] = safe_divide(data['Close'], shifted, 1.0) - 1
            # Rate of Change
            data[f'ROC_{window}'] = data['Close'].pct_change(window) * 100
        
        # Stochastic Oscillator
        low_14 = data['Low'].rolling(14).min()
        high_14 = data['High'].rolling(14).max()
        data['Stochastic_%K'] = 100 * safe_divide(data['Close'] - low_14, high_14 - low_14, 50)
        data['Stochastic_%D'] = data['Stochastic_%K'].rolling(3).mean().fillna(50)
        
        # Williams %R
        data['Williams_%R'] = -100 * safe_divide(high_14 - data['Close'], high_14 - low_14, -50)
        
        # 6. ATR (Average True Range)
        high_low = data['High'] - data['Low']
        high_close = np.abs(data['High'] - data['Close'].shift(1))
        low_close = np.abs(data['Low'] - data['Close'].shift(1))
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        data['ATR_14'] = true_range.rolling(14).mean().fillna(0)
        data['ATR_Ratio'] = safe_divide(data['ATR_14'], data['Close'], 0.01)
        
        # 7. SUPPORT/RESISTANCE
        data['Resistance_20'] = data['High'].rolling(20).max().fillna(data['Close'])
        data['Support_20'] = data['Low'].rolling(20).min().fillna(data['Close'])
        data['Resistance_Distance'] = safe_divide(data['Close'] - data['Resistance_20'], data['Close'], -0.1)
        data['Support_Distance'] = safe_divide(data['Close'] - data['Support_20'], data['Close'], 0.1)
        
        # 8. PRICE PATTERNS
        prev_close = data['Close'].shift(1).fillna(data['Close'])
        data['Gap_Up'] = ((data['Open'] > prev_close * 1.01) & (data['Open'] > 0)).astype(int)
        data['Gap_Down'] = ((data['Open'] < prev_close * 0.99) & (data['Open'] > 0)).astype(int)
        
        # Inside/Outside days
        data['Inside_Day'] = ((data['High'] < data['High'].shift(1)) & 
                             (data['Low'] > data['Low'].shift(1))).astype(int)
        data['Outside_Day'] = ((data['High'] > data['High'].shift(1)) & 
                              (data['Low'] < data['Low'].shift(1))).astype(int)
        
        # 9. SEASONALITY FEATURES
        if 'Date' in data.columns:
            try:
                dates = pd.to_datetime(data['Date'])
                data['Day_of_Week'] = dates.dt.dayofweek
                data['Month'] = dates.dt.month
                data['Week_of_Year'] = dates.dt.isocalendar().week
                
                # Month effects
                data['Month_End'] = dates.dt.is_month_end.astype(int)
                data['Month_Start'] = dates.dt.is_month_start.astype(int)
                
                # Day of month
                data['Day_of_Month'] = dates.dt.day
                data['Is_15th'] = (data['Day_of_Month'] == 15).astype(int)
            except Exception as e:
                pass
        
        # 10. LAGGED FEATURES
        for lag in [1, 2, 3, 5, 10]:
            data[f'Return_Lag_{lag}'] = data['Return'].shift(lag)
            data[f'Volume_Lag_{lag}'] = data['Volume'].shift(lag)
            data[f'Close_Lag_{lag}'] = data['Close'].shift(lag)
        
        # Rolling statistics
        data['Return_Std_20'] = data['Return'].rolling(20).std().fillna(0)
        data['Volume_Std_20'] = data['Volume'].rolling(20).std().fillna(0)
        
        # 11. INTERACTION FEATURES
        data['Volume_RSI_Interaction'] = data['Volume_Ratio'] * (data['RSI_14'] / 100)
        data['Vol_MA_Signal'] = data['Volatility_5d'] * data['MA5_Above_MA20']
        data['RSI_MACD_Interaction'] = (data['RSI_14'] / 100) * data['MACD']
        
        # 12. BINARY SIGNAL FEATURES
        data['Volume_Spike'] = (data['Volume_Ratio'] > 2).astype(int)
        data['High_Volatility'] = (data['Volatility_5d'] > 0.02).astype(int)
        data['Strong_Uptrend'] = ((data['MA_5'] > data['MA_20']) & 
                                  (data['MA_20'] > data['MA_50'])).astype(int)
        data['Strong_Downtrend'] = ((data['MA_5'] < data['MA_20']) & 
                                    (data['MA_20'] < data['MA_50'])).astype(int)
        
        # Handle any remaining NaN values
        data = data.fillna(method='ffill').fillna(method='bfill')
        
        # Final check - replace any remaining NaN with appropriate defaults
        for col in data.columns:
            if col != 'Date':
                if data[col].isna().any():
                    if 'Ratio' in col or 'Momentum' in col or 'ROC' in col:
                        data[col] = data[col].fillna(0.0)
                    elif col in ['Return', 'Volatility', 'Volume_Change', 'MACD', 'MACD_Signal', 'MACD_Histogram']:
                        data[col] = data[col].fillna(0.0)
                    elif 'RSI' in col or 'Stochastic' in col or 'Williams' in col:
                        data[col] = data[col].fillna(50.0)
                    elif col == 'Volume':
                        data[col] = data[col].fillna(1000000)
                    elif col in ['Open', 'High', 'Low', 'Close']:
                        data[col] = data[col].fillna(100.0)
                    else:
                        data[col] = data[col].fillna(0.0)
        
        return data
    except Exception as e:
        print(f"Feature creation error: {e}")
        import traceback
        traceback.print_exc()
        # Return minimal valid dataframe
        return pd.DataFrame({
            'Open': [100.0] * len(data) if hasattr(data, '__len__') else [100.0],
            'High': [101.0] * len(data) if hasattr(data, '__len__') else [101.0],
            'Low': [99.0] * len(data) if hasattr(data, '__len__') else [99.0],
            'Close': [100.0] * len(data) if hasattr(data, '__len__') else [100.0],
            'Volume': [1000000] * len(data) if hasattr(data, '__len__') else [1000000]
        })

def detect_anomalies(data):
    """Detect anomalies in the data using Isolation Forest"""
    try:
        features = ['Return', 'Volatility_5d', 'Volume_Ratio', 'RSI_14', 'High_Low_Range']
        available_features = [f for f in features if f in data.columns]
        
        if len(available_features) < 3:
            return pd.Series([0] * len(data), index=data.index)
        
        X = data[available_features].fillna(0).values
        
        # Fit Isolation Forest
        iso_forest = IsolationForest(contamination=0.1, random_state=42)
        anomalies = iso_forest.fit_predict(X)
        
        # Convert to binary (1 = anomaly, 0 = normal)
        anomaly_score = (anomalies == -1).astype(int)
        return pd.Series(anomaly_score, index=data.index)
    except:
        return pd.Series([0] * len(data), index=data.index)

# ---------------- Live data fetching ----------------
@rate_limiter
def get_live_stock_data_enhanced(ticker):
    """Fetch stock data with enhanced error handling - 10 YEARS DATA"""
    try:
        print(f"üìä Fetching 10 years of historical data for {ticker}...")
        
        # FIXED: Using the correct yfinance syntax
        strategies = [
            {"func": lambda t=ticker: yf.download(t, period="10y", interval="1d", progress=False, timeout=60), "name":"10y"},
            {"func": lambda t=ticker: yf.Ticker(t).history(period="10y", interval="1d"), "name":"ticker.history"},
        ]
        
        for strategy in strategies:
            try:
                print(f"  Trying strategy: {strategy['name']}")
                hist = strategy['func']()
                
                if isinstance(hist, pd.DataFrame) and (not hist.empty) and len(hist) > 200:
                    hist = hist.reset_index()
                    
                    # Handle date column
                    if 'Date' in hist.columns:
                        hist['Date'] = pd.to_datetime(hist['Date']).dt.strftime('%Y-%m-%d')
                    elif 'Datetime' in hist.columns:
                        hist['Date'] = pd.to_datetime(hist['Datetime']).dt.strftime('%Y-%m-%d')
                        hist = hist.drop(columns=['Datetime'])
                    
                    # Ensure we have all required columns
                    required = ['Open', 'High', 'Low', 'Close', 'Volume']
                    for col in required:
                        if col not in hist.columns:
                            print(f"    Warning: Missing {col} in data for {ticker}")
                            if col == 'Volume':
                                hist[col] = 1000000
                            else:
                                hist[col] = hist.get('Close', 100.0) if 'Close' in hist.columns else 100.0
                    
                    # Convert to numeric and handle NaNs
                    for col in required:
                        hist[col] = pd.to_numeric(hist[col], errors='coerce')
                        hist[col] = hist[col].ffill().bfill()
                        if hist[col].isna().any():
                            if col == 'Volume':
                                hist[col] = hist[col].fillna(1000000)
                            else:
                                hist[col] = hist[col].fillna(100.0)
                    
                    # Get current price
                    if 'Close' in hist.columns and len(hist) > 0:
                        current_price = float(hist['Close'].iloc[-1])
                    else:
                        current_price = 100.0
                    
                    print(f"‚úÖ Successfully fetched {len(hist)} days of data for {ticker}")
                    print(f"   Date range: {hist['Date'].iloc[0]} to {hist['Date'].iloc[-1]}")
                    print(f"   Current price: ${current_price:.2f}")
                    
                    # Check for splits
                    clean_data, has_split, split_info = detect_and_handle_splits(hist, ticker)
                    if has_split:
                        print(f"üö® IMPORTANT: {ticker} has stock splits")
                        print(f"   Using post-split data only for analysis")
                        current_price = float(clean_data['Close'].iloc[-1]) if len(clean_data) > 0 else current_price
                        return clean_data, current_price, None
                    
                    return hist, current_price, None
                    
            except Exception as e:
                print(f"    Strategy {strategy['name']} failed: {str(e)[:100]}...")
                if "429" in str(e):
                    time.sleep(10)
                continue
        
        # Fallback: generate synthetic data
        print(f"‚ö†Ô∏è Using fallback data for {ticker}")
        return generate_fallback_data(ticker, days=2520)  # 10 years
        
    except Exception as e:
        print(f"‚ùå Error fetching data for {ticker}: {e}")
        return generate_fallback_data(ticker, days=2520)

def generate_fallback_data(ticker, days=2520):
    """Generate fallback synthetic data without NaNs"""
    base_prices = {'AAPL':271,'MSFT':407,'GOOGL':172,'AMZN':178,'TSLA':175,'SPY':445}
    base_price = base_prices.get(ticker, 100.0)
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    dates = pd.date_range(start=start_date, end=end_date, freq='D')
    dates = dates[dates.dayofweek < 5]
    
    if len(dates) > days:
        dates = dates[-days:]
    
    # Generate realistic prices
    prices = [base_price]
    for i in range(1, len(dates)):
        change = random.gauss(0, 0.015)
        new_price = prices[-1] * (1 + change)
        new_price = max(new_price, base_price * 0.1)
        new_price = min(new_price, base_price * 3)
        prices.append(new_price)
    
    # Generate OHLC data
    opens, highs, lows = [], [], []
    for i, close in enumerate(prices):
        open_price = close * random.uniform(0.99, 1.01)
        high = max(open_price, close) * random.uniform(1.001, 1.03)
        low = min(open_price, close) * random.uniform(0.97, 0.999)
        
        opens.append(open_price)
        highs.append(high)
        lows.append(low)
    
    df = pd.DataFrame({
        'Date': dates.strftime('%Y-%m-%d'),
        'Open': opens,
        'High': highs,
        'Low': lows,
        'Close': prices,
        'Volume': [random.randint(1000000, 10000000) for _ in range(len(prices))]
    })
    
    print(f"üìä Generated {len(df)} days of fallback data for {ticker}")
    return df, prices[-1], None

# ---------------- OCHL Multi-Algorithm Predictor ----------------
class OCHLPredictor:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_scaler = RobustScaler()
        self.feature_columns = []
        self.targets = ['Open', 'Close', 'High', 'Low']
        self.historical_performance = {}
        self.prediction_history = {}
        self.risk_metrics = {}
        self.algorithm_weights = {}
        self.last_training_date = None
        self.is_fitted = False
        self.split_info = {}  # Store split information for each symbol
        
    def get_model_path(self, symbol, target, algorithm):
        safe_symbol = "".join([c for c in symbol if c.isalnum() or c in "-_"]).upper()
        safe_target = target.lower()
        safe_algo = algorithm.lower().replace(" ", "_")
        return os.path.join(MODELS_DIR, f"{safe_symbol}_{safe_target}_{safe_algo}.joblib")
    
    def get_history_path(self, symbol):
        safe_symbol = "".join([c for c in symbol if c.isalnum() or c in "-_"]).upper()
        return os.path.join(HISTORY_DIR, f"{symbol}_history.json")
    
    def get_scaler_path(self, symbol):
        safe_symbol = "".join([c for c in symbol if c.isalnum() or c in "-_"]).upper()
        return os.path.join(MODELS_DIR, f"{symbol}_scalers.joblib")
    
    def save_models(self, symbol):
        try:
            # Save feature scaler separately
            scaler_data = {
                'feature_scaler': self.feature_scaler,
                'scalers': self.scalers,
                'feature_columns': self.feature_columns,
                'split_info': self.split_info.get(symbol)
            }
            scaler_path = self.get_scaler_path(symbol)
            joblib.dump(scaler_data, scaler_path)
            print(f"üíæ Saved scalers for {symbol}")
            
            # Save individual models
            for target in self.targets:
                if target in self.models:
                    for algo, model in self.models[target].items():
                        if model is not None:
                            model_data = {
                                'model': model,
                                'window_size': 30,
                                'target': target,
                                'algorithm': algo
                            }
                            path = self.get_model_path(symbol, target, algo)
                            joblib.dump(model_data, path)
            
            # Save history
            history_data = {
                'historical_performance': self.historical_performance,
                'prediction_history': self.prediction_history,
                'risk_metrics': self.risk_metrics,
                'algorithm_weights': self.algorithm_weights,
                'last_training_date': self.last_training_date,
                'feature_columns': self.feature_columns,
                'split_info': self.split_info.get(symbol)
            }
            
            with open(self.get_history_path(symbol), 'w') as f:
                json.dump(history_data, f, default=str)
                
            print(f"üíæ Saved models and history for {symbol}")
            return True
        except Exception as e:
            print(f"Error saving models: {e}")
            return False
    
    def load_models(self, symbol):
        try:
            loaded_models = {target: {} for target in self.targets}
            loaded_scalers = {}
            
            # First try to load scalers
            scaler_path = self.get_scaler_path(symbol)
            if os.path.exists(scaler_path):
                try:
                    scaler_data = joblib.load(scaler_path)
                    self.feature_scaler = scaler_data.get('feature_scaler', RobustScaler())
                    self.scalers = scaler_data.get('scalers', {})
                    self.feature_columns = scaler_data.get('feature_columns', [])
                    self.split_info[symbol] = scaler_data.get('split_info')
                    
                    # Check if feature scaler is properly fitted
                    if hasattr(self.feature_scaler, "median_"):
                        self.is_fitted = True
                        print(f"‚úÖ Feature scaler properly loaded for {symbol}")
                    else:
                        self.is_fitted = False
                        print(f"‚ö†Ô∏è Feature scaler not fitted in loaded data for {symbol}")
                except Exception as e:
                    print(f"Error loading scalers: {e}")
                    self.is_fitted = False
            
            algorithms = ['linear_regression', 'ridge', 'lasso', 'svr', 'random_forest', 'gradient_boosting', 'arima', 'neural_network']
            
            models_loaded = False
            
            for target in self.targets:
                for algo in algorithms:
                    path = self.get_model_path(symbol, target, algo)
                    if os.path.exists(path):
                        try:
                            model_data = joblib.load(path)
                            loaded_models[target][algo] = model_data['model']
                            models_loaded = True
                        except Exception as e:
                            print(f"Error loading {target}-{algo}: {e}")
                            loaded_models[target][algo] = None
                    else:
                        loaded_models[target][algo] = None
            
            if models_loaded:
                self.models = loaded_models
                
                # Load history
                history_path = self.get_history_path(symbol)
                if os.path.exists(history_path):
                    try:
                        with open(history_path, 'r') as f:
                            history_data = json.load(f)
                        
                        self.historical_performance = history_data.get('historical_performance', {})
                        self.prediction_history = history_data.get('prediction_history', {})
                        self.risk_metrics = history_data.get('risk_metrics', {})
                        self.algorithm_weights = history_data.get('algorithm_weights', {})
                        self.last_training_date = history_data.get('last_training_date')
                        self.split_info[symbol] = history_data.get('split_info')
                        
                    except Exception as e:
                        print(f"Error loading history: {e}")
                
                print(f"‚úÖ Loaded existing models for {symbol}")
                return True
            else:
                print(f"‚ùå No models found for {symbol}")
                return False
                
        except Exception as e:
            print(f"Error loading models: {e}")
            return False
    
    def validate_training_data(self, X, y):
        """Validate training data before model training - FIXED VERSION"""
        if X is None or y is None:
            return False, "No data"
        
        if len(X) < 50:
            return False, f"Insufficient samples: {len(X)}"
        
        # Check for NaN/inf
        if np.any(np.isnan(X)) or np.any(np.isnan(y)):
            return False, "NaN values detected"
        
        # Check for infinite values
        if np.any(np.isinf(X)) or np.any(np.isinf(y)):
            return False, "Infinite values detected"
        
        # Check if all values are the same (no variation)
        if len(np.unique(y)) < 2:
            return False, "No variation in target values"
        
        return True, "Data valid"
    
    def clean_training_data(self, X, y, algorithm):
        """Clean training data for sensitive algorithms - SIMPLIFIED"""
        if len(X) == 0 or len(y) == 0:
            return X, y
        
        # Remove rows with NaN or Inf
        mask = ~np.any(np.isnan(X), axis=1) & ~np.isnan(y) & ~np.any(np.isinf(X), axis=1) & ~np.isinf(y)
        X_clean = X[mask]
        y_clean = y[mask]
        
        # For sensitive algorithms, remove extreme outliers
        if algorithm in ['linear_regression', 'ridge', 'lasso', 'svr', 'neural_network']:
            if len(y_clean) > 10:
                y_mean, y_std = np.mean(y_clean), np.std(y_clean)
                if y_std > 0:
                    mask = np.abs(y_clean - y_mean) < 3 * y_std
                    X_clean = X_clean[mask]
                    y_clean = y_clean[mask]
        
        return X_clean, y_clean
    
    def prepare_training_data(self, data):
        try:
            print(f"\nüìã Preparing training data...")
            print(f"   Initial data: {len(data)} rows")
            
            data_with_features = create_advanced_features(data)
            print(f"   Features created: {len(data_with_features.columns)}")
            
            if len(data_with_features) < 100:
                print(f"‚ùå Insufficient data ({len(data_with_features)} rows)")
                return None, None, None
            
            numeric_cols = [col for col in data_with_features.columns 
                          if col != 'Date' and pd.api.types.is_numeric_dtype(data_with_features[col])]
            
            feature_candidates = [col for col in numeric_cols if col not in self.targets]
            
            if not self.feature_columns:
                if len(feature_candidates) > 30:
                    variances = data_with_features[feature_candidates].var()
                    variances = variances.fillna(0)
                    self.feature_columns = variances.nlargest(30).index.tolist()
                else:
                    self.feature_columns = feature_candidates
            
            if not self.feature_columns:
                default_features = ['Return', 'Volatility_5d', 'Volume_Ratio', 'RSI_14', 'High_Low_Range', 
                                   'MA_5', 'MA_10', 'MA_20', 'BB_Position', 'Momentum_5',
                                   'MACD', 'ATR_Ratio', 'Volume_Price_Trend']
                self.feature_columns = [f for f in default_features if f in data_with_features.columns]
                if not self.feature_columns:
                    self.feature_columns = list(data_with_features.columns[:15])
            
            print(f"   Using {len(self.feature_columns)} features")
            
            X_data = {}
            y_data = {}
            
            for target in self.targets:
                if target not in data_with_features.columns:
                    data_with_features[target] = data_with_features['Close']
                
                data_with_features[target] = data_with_features[target].ffill().bfill().fillna(data_with_features['Close'].mean())
                
                X_list = []
                y_list = []
                window_size = 30
                
                if len(data_with_features) < window_size + 20:
                    continue
                
                missing_features = [f for f in self.feature_columns if f not in data_with_features.columns]
                if missing_features:
                    for f in missing_features:
                        data_with_features[f] = 0
                
                samples_count = 0
                for i in range(window_size, len(data_with_features) - 1):
                    features = data_with_features[self.feature_columns].iloc[i-window_size:i]
                    
                    if features.isna().any().any():
                        features = features.fillna(0)
                    
                    features_array = features[self.feature_columns].values
                    features_flat = features_array.flatten()
                    
                    expected_dim = len(self.feature_columns) * window_size
                    if features_flat.shape[0] != expected_dim:
                        continue
                    
                    target_value = data_with_features[target].iloc[i+1]
                    
                    if pd.isna(target_value):
                        continue
                    
                    target_mean = data_with_features[target].mean()
                    target_std = data_with_features[target].std()
                    if target_std > 0 and abs(target_value - target_mean) > 5 * target_std:
                        continue
                    
                    X_list.append(features_flat)
                    y_list.append(target_value)
                    samples_count += 1
                
                if samples_count > 50:
                    X_data[target] = np.array(X_list)
                    y_data[target] = np.array(y_list)
                    print(f"   {target}: {samples_count} samples")
                else:
                    print(f"   {target}: Insufficient samples ({samples_count})")
                    X_data[target] = None
                    y_data[target] = None
            
            valid_targets = [t for t in self.targets if t in X_data and X_data[t] is not None and len(X_data[t]) > 0]
            if len(valid_targets) == 0:
                print("‚ùå No valid training data")
                return None, None, None
            
            print(f"‚úÖ Prepared data for {len(valid_targets)} targets")
            return X_data, y_data, data_with_features
            
        except Exception as e:
            print(f"Error preparing training data: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None
    
    def train_algorithm(self, X, y, algorithm, target):
        try:
            # Validate data first
            is_valid, msg = self.validate_training_data(X, y)
            if not is_valid:
                print(f"      ‚ùå {algorithm}: {msg}")
                return None
            
            # Clean data based on algorithm sensitivity
            X_clean, y_clean = self.clean_training_data(X, y, algorithm)
            
            min_samples = self.get_min_samples_for_algorithm(algorithm)
            if len(X_clean) < min_samples:
                print(f"      ‚ö†Ô∏è {algorithm}: Insufficient clean data ({len(X_clean)} < {min_samples})")
                return None
            
            # Handle NaN values
            X_clean = np.nan_to_num(X_clean, nan=0.0)
            y_mean = np.nanmean(y_clean) if not np.all(np.isnan(y_clean)) else 0.0
            y_clean = np.nan_to_num(y_clean, nan=y_mean)
            
            if algorithm == 'linear_regression':
                try:
                    model = LinearRegression()
                    model.fit(X_clean, y_clean)
                    return model
                except Exception as e:
                    print(f"      ‚ùå {algorithm}: Error - {str(e)[:50]}")
                    return None
                
            elif algorithm == 'ridge':
                try:
                    model = Ridge(alpha=1.0, random_state=42, max_iter=10000)
                    model.fit(X_clean, y_clean)
                    return model
                except Exception as e:
                    print(f"      ‚ùå {algorithm}: Error - {str(e)[:50]}")
                    return None
                
            elif algorithm == 'lasso':
                try:
                    model = Lasso(alpha=0.01, random_state=42, max_iter=10000)
                    model.fit(X_clean, y_clean)
                    return model
                except Exception as e:
                    print(f"      ‚ùå {algorithm}: Error - {str(e)[:50]}")
                    return None
                
            elif algorithm == 'svr':
                try:
                    # Use simpler SVR for better stability
                    model = SVR(kernel='rbf', C=1.0, epsilon=0.1, max_iter=1000)
                    
                    if len(X_clean) > 1000:
                        idx = np.random.choice(len(X_clean), min(1000, len(X_clean)), replace=False)
                        model.fit(X_clean[idx], y_clean[idx])
                    else:
                        model.fit(X_clean, y_clean)
                    return model
                except Exception as e:
                    print(f"      ‚ùå {algorithm}: Error - {str(e)[:50]}")
                    return None
                
            elif algorithm == 'random_forest':
                try:
                    model = RandomForestRegressor(
                        n_estimators=100,
                        max_depth=10,
                        min_samples_split=5,
                        min_samples_leaf=2,
                        max_features='sqrt',
                        random_state=42,
                        n_jobs=-1
                    )
                    model.fit(X_clean, y_clean)
                    return model
                except Exception as e:
                    print(f"      ‚ùå {algorithm}: Error - {str(e)[:50]}")
                    return None
                
            elif algorithm == 'gradient_boosting':
                try:
                    model = GradientBoostingRegressor(
                        n_estimators=50,
                        learning_rate=0.1,
                        max_depth=4,
                        min_samples_split=5,
                        min_samples_leaf=2,
                        random_state=42
                    )
                    model.fit(X_clean, y_clean)
                    return model
                except Exception as e:
                    print(f"      ‚ùå {algorithm}: Error - {str(e)[:50]}")
                    return None
                
            elif algorithm == 'neural_network':
                try:
                    model = MLPRegressor(
                        hidden_layer_sizes=(50, 25),
                        activation='relu',
                        solver='adam',
                        alpha=0.01,
                        max_iter=500,
                        random_state=42,
                        early_stopping=True,
                        validation_fraction=0.2
                    )
                    model.fit(X_clean, y_clean)
                    return model
                except Exception as e:
                    print(f"      ‚ùå {algorithm}: Error - {str(e)[:50]}")
                    return None
                
            elif algorithm == 'arima':
                if len(y_clean) > 100:
                    try:
                        y_clean_series = pd.Series(y_clean)
                        
                        model = pm.auto_arima(
                            y_clean_series,
                            start_p=0, start_q=0,
                            max_p=2, max_q=2, m=1,
                            seasonal=False,
                            trace=False,
                            error_action='ignore',
                            suppress_warnings=True,
                            maxiter=20,
                            stepwise=True
                        )
                        return model
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è ARIMA auto failed: {str(e)[:50]}")
                        try:
                            model = StatsmodelsARIMA(y_clean_series, order=(1,0,0))
                            model_fit = model.fit()
                            return model_fit
                        except:
                            print(f"      ‚ùå ARIMA simple failed")
                            return None
                else:
                    print(f"      ‚ö†Ô∏è ARIMA: Insufficient data ({len(y_clean)} samples)")
                    return None
                    
            return None
        except Exception as e:
            print(f"      ‚ùå {algorithm}: Unexpected error - {str(e)[:50]}")
            return None
    
    def get_min_samples_for_algorithm(self, algorithm):
        """Get minimum samples required for each algorithm"""
        if algorithm in ['arima']:
            return 50  # ARIMA needs less data
        elif algorithm in ['linear_regression', 'ridge', 'lasso']:
            return 50  # Linear models
        elif algorithm in ['svr', 'neural_network']:
            return 100  # Complex models need more
        else:
            return 50
    
    def train_all_models(self, data, symbol):
        try:
            print(f"\nüî® TRAINING MODELS FOR {symbol}")
            print(f"="*60)
            
            # Check for and handle splits BEFORE training
            clean_data, has_split, split_info = detect_and_handle_splits(data, symbol)
            if has_split:
                print(f"üö® IMPORTANT: Training on POST-SPLIT DATA ONLY")
                print(f"   Split info: {split_info}")
                self.split_info[symbol] = split_info
                data = clean_data
            
            X_data, y_data, data_with_features = self.prepare_training_data(data)
            
            if X_data is None or y_data is None:
                return False, "Insufficient data for training"
            
            algorithms = ['linear_regression', 'ridge', 'lasso', 'svr', 'random_forest', 'gradient_boosting', 'arima', 'neural_network']
            self.models = {target: {} for target in self.targets}
            self.scalers = {}
            self.algorithm_weights = {target: {} for target in self.targets}
            
            print(f"üìä Training {len(algorithms)} algorithms:")
            print(f"   Algorithms: {', '.join(algorithms)}")
            
            # Combine all features for scaling
            all_features = []
            for target in self.targets:
                if target in X_data and X_data[target] is not None:
                    all_features.append(X_data[target])
            
            if all_features:
                all_features_array = np.vstack(all_features)
                # Apply clipping to prevent extreme values
                all_features_array = np.clip(all_features_array, 
                                           np.percentile(all_features_array, 1, axis=0),
                                           np.percentile(all_features_array, 99, axis=0))
                self.feature_scaler.fit(all_features_array)
                print(f"‚úÖ Fitted RobustScaler with {all_features_array.shape[0]} samples")
            else:
                print("‚ö†Ô∏è No features available for scaling")
                self.feature_scaler = RobustScaler()  # Reset
            
            targets_trained = 0
            
            for target in self.targets:
                print(f"\n   üéØ Training {target}...")
                
                if target not in X_data or X_data[target] is None:
                    print(f"   ‚ùå Skipping: No data")
                    continue
                
                X = X_data[target]
                y = y_data[target]
                
                if len(X) < 50 or len(y) < 50:
                    print(f"   ‚ùå Skipping: Insufficient samples ({len(X)})")
                    continue
                
                try:
                    X_scaled = self.feature_scaler.transform(X)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Feature scaling failed, using unscaled: {e}")
                    X_scaled = X
                
                # CRITICAL FIX: DO NOT SCALE THE TARGET PRICES!
                # Models should predict actual prices, not scaled values
                # This was causing the insane predictions
                y_scaled = y  # Keep as actual prices
                self.scalers[target] = None  # No scaler needed for target
                
                successful_models = 0
                for algo in algorithms:
                    print(f"      Training {algo}...", end=" ")
                    model = self.train_algorithm(X_scaled, y_scaled, algo, target)
                    self.models[target][algo] = model
                    if model is not None:
                        print(f"‚úÖ")
                        successful_models += 1
                    else:
                        print(f"‚ùå")
                
                if successful_models > 0:
                    targets_trained += 1
                    print(f"   ‚úÖ Trained {successful_models}/{len(algorithms)} algorithms for {target}")
                else:
                    print(f"   ‚ùå Failed all algorithms for {target}")
            
            if targets_trained == 0:
                print(f"\n‚ùå Failed to train any targets")
                return False, "Failed to train any models"
            
            print(f"\nüìà Calculating performance metrics...")
            self.calculate_historical_performance(X_data, y_data, data_with_features)
            
            print(f"üìä Calculating risk metrics...")
            self.calculate_risk_metrics(data_with_features)
            
            self.update_prediction_history(symbol, data_with_features)
            
            self.last_training_date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            self.is_fitted = True
            
            self.save_models(symbol)
            
            print(f"\n‚úÖ TRAINING COMPLETE")
            print(f"   Targets trained: {targets_trained}/{len(self.targets)}")
            print(f"   Total models trained: {sum(len(self.models[t]) for t in self.targets)}")
            print(f"   Features: {len(self.feature_columns)}")
            if has_split:
                print(f"   ‚ö†Ô∏è Split handling: POST-SPLIT DATA ONLY")
            print(f"="*60)
            
            return True, f"Trained models for {targets_trained} targets"
            
        except Exception as e:
            print(f"Error training all models: {e}")
            import traceback
            traceback.print_exc()
            return False, str(e)
    
    def calculate_historical_performance(self, X_data, y_data, data_with_features):
        try:
            performance = {}
            
            for target in self.targets:
                if target not in self.models or target not in X_data:
                    continue
                
                X = X_data[target]
                y = y_data[target]
                
                if X is None or y is None or len(X) < 50:
                    continue
                
                tscv = TimeSeriesSplit(n_splits=3)
                fold_scores = {algo: {'rmse': [], 'mae': [], 'r2': [], 'direction': []} 
                              for algo in self.models[target].keys() if self.models[target][algo] is not None}
                
                if not fold_scores:  # No models trained for this target
                    continue
                
                for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
                    if fold >= 2:
                        break
                        
                    X_train, X_test = X[train_idx], X[test_idx]
                    y_train, y_test = y[train_idx], y[test_idx]
                    
                    if len(X_test) < 5 or len(y_test) < 5:
                        continue
                    
                    try:
                        X_train_scaled = self.feature_scaler.transform(X_train)
                        X_test_scaled = self.feature_scaler.transform(X_test)
                    except:
                        X_train_scaled = X_train
                        X_test_scaled = X_test
                    
                    # CRITICAL FIX: Use percentage returns for performance metrics
                    # This gives more realistic R¬≤ and error metrics
                    y_train_returns = np.diff(y_train) / y_train[:-1]
                    y_test_returns = np.diff(y_test) / y_test[:-1]
                    
                    # Need to adjust X to match returns
                    if len(y_train_returns) > 0 and len(y_test_returns) > 0:
                        X_train_scaled = X_train_scaled[1:]  # Skip first element
                        X_test_scaled = X_test_scaled[1:]
                        
                        if len(X_train_scaled) != len(y_train_returns):
                            min_len = min(len(X_train_scaled), len(y_train_returns))
                            X_train_scaled = X_train_scaled[:min_len]
                            y_train_returns = y_train_returns[:min_len]
                        
                        if len(X_test_scaled) != len(y_test_returns):
                            min_len = min(len(X_test_scaled), len(y_test_returns))
                            X_test_scaled = X_test_scaled[:min_len]
                            y_test_returns = y_test_returns[:min_len]
                    
                    for algo, model in self.models[target].items():
                        if model is None or algo not in fold_scores:
                            continue
                        
                        try:
                            if algo == 'arima':
                                if len(y_train) > 30:
                                    try:
                                        temp_arima = pm.auto_arima(
                                            y_train,
                                            start_p=0, start_q=0,
                                            max_p=2, max_q=2,
                                            seasonal=False,
                                            trace=False,
                                            error_action='ignore'
                                        )
                                        predictions = temp_arima.predict(n_periods=len(y_test))
                                    except:
                                        predictions = np.zeros_like(y_test)
                                else:
                                    predictions = np.zeros_like(y_test)
                            else:
                                # Train a fresh model for cross-validation
                                if algo == 'linear_regression':
                                    fold_model = LinearRegression()
                                elif algo == 'ridge':
                                    fold_model = Ridge(alpha=1.0, random_state=42)
                                elif algo == 'lasso':
                                    fold_model = Lasso(alpha=0.01, random_state=42)
                                elif algo == 'svr':
                                    fold_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)
                                elif algo == 'random_forest':
                                    fold_model = RandomForestRegressor(n_estimators=50, random_state=42)
                                elif algo == 'gradient_boosting':
                                    fold_model = GradientBoostingRegressor(n_estimators=30, random_state=42)
                                elif algo == 'neural_network':
                                    fold_model = MLPRegressor(hidden_layer_sizes=(30,), random_state=42, max_iter=300)
                                else:
                                    continue
                                
                                fold_model.fit(X_train_scaled, y_train)
                                predictions = fold_model.predict(X_test_scaled)
                            
                            if np.any(np.isnan(predictions)):
                                predictions = np.nan_to_num(predictions, nan=np.nanmean(predictions))
                            
                            min_len = min(len(predictions), len(y_test))
                            predictions = predictions[:min_len]
                            actuals = y_test[:min_len]
                            
                            if min_len > 0:
                                # Calculate percentage errors (more meaningful for stocks)
                                percent_errors = np.abs((predictions - actuals) / actuals) * 100
                                percent_errors = percent_errors[actuals > 0]  # Filter out zero prices
                                
                                if len(percent_errors) > 0:
                                    mape = np.mean(percent_errors)
                                else:
                                    mape = 100
                                
                                # Direction accuracy
                                if len(actuals) > 1:
                                    actual_direction = np.diff(actuals) > 0
                                    pred_direction = np.diff(predictions) > 0
                                    min_dir_len = min(len(actual_direction), len(pred_direction))
                                    if min_dir_len > 0:
                                        direction_acc = np.mean(actual_direction[:min_dir_len] == pred_direction[:min_dir_len]) * 100
                                    else:
                                        direction_acc = 50  # Neutral baseline
                                else:
                                    direction_acc = 50
                                
                                # Normalize R¬≤ (it's often negative for stock predictions)
                                rmse = np.sqrt(mean_squared_error(actuals, predictions))
                                mae = mean_absolute_error(actuals, predictions)
                                
                                # Stock price R¬≤ is often low, adjust expectations
                                r2 = r2_score(actuals, predictions)
                                normalized_r2 = max(0, (r2 + 1) / 2)  # Normalize to 0-1 range
                                
                                fold_scores[algo]['rmse'].append(rmse)
                                fold_scores[algo]['mae'].append(mae)
                                fold_scores[algo]['r2'].append(normalized_r2)  # Use normalized
                                fold_scores[algo]['direction'].append(direction_acc)
                                fold_scores[algo]['mape'].append(mape)
                            
                        except Exception as e:
                            print(f"      Error in {algo} CV: {e}")
                            continue
                
                target_performance = {}
                target_weights = {}
                
                for algo in self.models[target].keys():
                    if algo not in fold_scores or not fold_scores[algo]['direction']:
                        continue
                    
                    avg_direction = np.mean(fold_scores[algo]['direction'])
                    avg_mape = np.mean(fold_scores[algo]['mape'])
                    avg_r2 = np.mean(fold_scores[algo]['r2'])
                    
                    # REALISTIC CONFIDENCE: Direction accuracy is most important
                    # For stocks, 55%+ direction accuracy is good
                    if avg_direction > 55:
                        direction_score = min(100, (avg_direction - 50) * 2)  # Scale 50-75% to 0-50%
                    else:
                        direction_score = max(0, (avg_direction - 45) * 2)  # Scale 45-50% to 0-10%
                    
                    # MAPE penalty: lower is better
                    mape_score = max(0, 100 - min(avg_mape, 100))
                    
                    # Combined confidence
                    confidence = (
                        direction_score * 0.6 +  # Direction accuracy is 60% of confidence
                        mape_score * 0.3 +        # MAPE is 30%
                        avg_r2 * 100 * 0.1        # R¬≤ is 10%
                    )
                    confidence = min(max(confidence, 0), 100)
                    
                    target_performance[algo] = {
                        'direction_accuracy': float(avg_direction),
                        'mape': float(avg_mape),
                        'r2': float(avg_r2),
                        'confidence': float(confidence),
                        'fold_count': len(fold_scores[algo]['direction'])
                    }
                    
                    # Weight based mostly on direction accuracy
                    if avg_direction > 50:
                        weight = (avg_direction - 50) / 25  # Scale 50-75% to 0-1
                    else:
                        weight = 0.1  # Minimum weight
                    
                    target_weights[algo] = max(weight, 0.1)
                
                performance[target] = target_performance
                self.algorithm_weights[target] = target_weights
            
            self.historical_performance = performance
            return performance
            
        except Exception as e:
            print(f"Error calculating performance: {e}")
            return {}
    
    def calculate_risk_metrics(self, data):
        try:
            if 'Close' not in data.columns:
                self.risk_metrics = {}
                return
            
            returns = data['Close'].pct_change().dropna()
            
            if len(returns) < 10:
                self.risk_metrics = {}
                return
            
            returns = returns.fillna(0)
            
            returns_mean, returns_std = returns.mean(), returns.std()
            if returns_std > 0:
                returns = returns[np.abs(returns - returns_mean) < 3 * returns_std]
            
            if len(returns) < 10:
                self.risk_metrics = {}
                return
            
            volatility = returns.std() * np.sqrt(252)
            sharpe_ratio = (returns.mean() * 252) / (returns.std() * np.sqrt(252)) if returns.std() > 0 else 0
            
            var_95 = np.percentile(returns, 5) * 100 if len(returns) >= 20 else 0
            var_99 = np.percentile(returns, 1) * 100 if len(returns) >= 100 else 0
            
            cvar_95 = returns[returns <= np.percentile(returns, 5)].mean() * 100 if len(returns) >= 20 else 0
            cvar_99 = returns[returns <= np.percentile(returns, 1)].mean() * 100 if len(returns) >= 100 else 0
            
            cumulative = (1 + returns).cumprod()
            rolling_max = cumulative.expanding().max()
            drawdown = (cumulative - rolling_max) / rolling_max.replace(0, 1e-10)
            max_drawdown = drawdown.min() * 100 if len(drawdown) > 0 else 0
            
            skewness = returns.skew() if len(returns) > 2 else 0
            kurtosis = returns.kurtosis() if len(returns) > 3 else 0
            
            anomaly_score = detect_anomalies(data).mean() * 100
            
            try:
                market_volatility = 0.15
                beta = volatility / market_volatility if market_volatility > 0 else 1.0
            except:
                beta = 1.0
            
            self.risk_metrics = {
                'volatility': float(volatility * 100),
                'sharpe_ratio': float(sharpe_ratio),
                'var_95': float(var_95),
                'var_99': float(var_99),
                'cvar_95': float(cvar_95),
                'cvar_99': float(cvar_99),
                'max_drawdown': float(max_drawdown),
                'skewness': float(skewness),
                'kurtosis': float(kurtosis),
                'anomaly_score': float(anomaly_score),
                'beta': float(beta),
                'total_returns': float(returns.mean() * 252 * 100),
                'positive_days': float((returns > 0).mean() * 100),
                'negative_days': float((returns < 0).mean() * 100)
            }
            
        except Exception as e:
            print(f"Error calculating risk metrics: {e}")
            self.risk_metrics = {}
    
    def update_prediction_history(self, symbol, data):
        try:
            if symbol not in self.prediction_history:
                self.prediction_history[symbol] = []
            
            latest_actuals = {
                'date': data['Date'].iloc[-1] if 'Date' in data.columns else datetime.now().strftime('%Y-%m-%d'),
                'actual_open': float(data['Open'].iloc[-1]) if 'Open' in data.columns else 0,
                'actual_high': float(data['High'].iloc[-1]) if 'High' in data.columns else 0,
                'actual_low': float(data['Low'].iloc[-1]) if 'Low' in data.columns else 0,
                'actual_close': float(data['Close'].iloc[-1]) if 'Close' in data.columns else 0,
                'volume': float(data['Volume'].iloc[-1]) if 'Volume' in data.columns else 0
            }
            
            self.prediction_history[symbol].append(latest_actuals)
            if len(self.prediction_history[symbol]) > 100:
                self.prediction_history[symbol] = self.prediction_history[symbol][-100:]
                
        except Exception as e:
            print(f"Error updating prediction history: {e}")
    
    def get_reliable_predictions(self, symbol, data):
        """Get predictions with enhanced reliability - FIXED UNPACKING"""
        try:
            # Try primary predictor
            result = self.predict_ochl(symbol, data)
            
            # Check if result is tuple (result, error) or just result
            if isinstance(result, tuple) and len(result) == 2:
                predictions, error = result
                if error:
                    print(f"‚ö†Ô∏è Primary predictor failed: {error}")
                    return self.get_conservative_fallback(data)
                return predictions
            else:
                # Already just the predictions
                return result
                
        except Exception as e:
            print(f"‚ùå Error in get_reliable_predictions: {e}")
            return self.get_conservative_fallback(data)
    
    def get_conservative_fallback(self, data):
        """Get conservative fallback predictions when models fail"""
        try:
            current_close = data['Close'].iloc[-1] if 'Close' in data.columns else 100.0
            
            # Conservative predictions based on recent trends
            recent_returns = []
            if 'Close' in data.columns and len(data) > 5:
                recent_prices = data['Close'].iloc[-5:].values
                if len(recent_prices) >= 2:
                    for i in range(1, len(recent_prices)):
                        recent_returns.append((recent_prices[i] - recent_prices[i-1]) / recent_prices[i-1])
            
            avg_return = np.mean(recent_returns) if recent_returns else 0
            
            # Very conservative predictions
            predictions = {}
            for target in self.targets:
                if target == 'High':
                    pred = current_close * (1 + min(avg_return + 0.01, 0.03))
                elif target == 'Low':
                    pred = current_close * (1 + max(avg_return - 0.01, -0.03))
                elif target == 'Open':
                    pred = current_close * (1 + avg_return * 0.5)
                else:  # Close
                    pred = current_close * (1 + avg_return)
                
                predictions[target] = pred
            
            # Ensure OHLC constraints
            pred_open = predictions.get("Open", current_close)
            pred_close = predictions.get("Close", current_close)
            pred_high = max(predictions.get("High", current_close * 1.01), pred_open, pred_close)
            pred_low = min(predictions.get("Low", current_close * 0.99), pred_open, pred_close)
            
            predictions["High"] = pred_high
            predictions["Low"] = pred_low
            
            # Apply bounds
            max_change = 0.05  # Very conservative
            for target in predictions:
                predictions[target] = max(predictions[target], current_close * (1 - max_change))
                predictions[target] = min(predictions[target], current_close * (1 + max_change))
            
            result = {
                'predictions': predictions,
                'confidence_scores': {target: 60.0 for target in self.targets},
                'confidence_metrics': {
                    'overall_confidence': 60.0,
                    'confidence_level': 'MEDIUM',
                    'confidence_color': 'warning'
                },
                'risk_alerts': [{
                    'level': 'üü° HIGH',
                    'type': 'Conservative Mode',
                    'message': 'Using conservative predictions',
                    'details': 'Primary models produced limited valid predictions'
                }],
                'current_prices': {
                    'open': float(data['Open'].iloc[-1]) if 'Open' in data.columns else current_close * 0.995,
                    'high': float(data['High'].iloc[-1]) if 'High' in data.columns else current_close * 1.015,
                    'low': float(data['Low'].iloc[-1]) if 'Low' in data.columns else current_close * 0.985,
                    'close': float(current_close)
                },
                'fallback': True
            }
            
            return result
            
        except Exception as e:
            print(f"Conservative fallback error: {e}")
            # Ultimate fallback
            current_close = data['Close'].iloc[-1] if 'Close' in data.columns else 100.0
            predictions = {target: float(current_close) for target in self.targets}
            
            result = {
                'predictions': predictions,
                'confidence_scores': {target: 50.0 for target in self.targets},
                'confidence_metrics': {
                    'overall_confidence': 50.0,
                    'confidence_level': 'LOW',
                    'confidence_color': 'danger'
                },
                'risk_alerts': [{
                    'level': 'üî¥ CRITICAL',
                    'type': 'Emergency Fallback',
                    'message': 'Using emergency fallback predictions',
                    'details': 'All prediction systems unavailable'
                }],
                'current_prices': {
                    'open': float(current_close * 0.995),
                    'high': float(current_close * 1.015),
                    'low': float(current_close * 0.985),
                    'close': float(current_close)
                },
                'fallback': True
            }
            
            return result
    
    def predict_ochl(self, symbol, data):
        """PREDICT WITH DETAILED OUTPUT FOR ALL MODELS - FIXED VERSION"""
        try:
            print(f"\n{'='*70}")
            print(f"ü§ñ PREDICTING OCHL FOR {symbol}")
            print(f"{'='*70}")
            
            # Check for splits in the data
            clean_data, has_split, split_info = detect_and_handle_splits(data, symbol)
            if has_split:
                print(f"üö® CRITICAL: {symbol} has stock splits")
                print(f"   Using post-split data only for prediction")
                print(f"   Split info: {split_info}")
                data = clean_data
            
            # Check if models exist
            if not self.models or all(len(self.models.get(target, {})) == 0 for target in self.targets):
                print(f"‚ö†Ô∏è No models available")
                return self.get_conservative_fallback(data)
            
            X_data, _, data_with_features = self.prepare_training_data(data)
            
            if X_data is None:
                return self.get_conservative_fallback(data)
            
            predictions = {}
            confidence_scores = {}
            algorithm_predictions = {}
            
            current_close = data_with_features['Close'].iloc[-1] if 'Close' in data_with_features.columns else 100.0
            print(f"üìä Current Price: ${current_close:.2f}")
            
            total_models_available = sum(len(self.models.get(target, {})) for target in self.targets)
            print(f"üìà Models available: {total_models_available} total")
            print()
            
            for target in self.targets:
                print(f"\nüéØ PREDICTING {target.upper()}")
                print(f"   {'‚îÄ'*50}")
                
                if target not in self.models or not self.models[target]:
                    print(f"   ‚ùå No models for this target")
                    predictions[target] = current_close
                    confidence_scores[target] = 50
                    algorithm_predictions[target] = {'ensemble': float(current_close)}
                    continue
            
                if target not in X_data or X_data[target] is None:
                    print(f"   ‚ùå No data for this target")
                    predictions[target] = current_close
                    confidence_scores[target] = 50
                    algorithm_predictions[target] = {'ensemble': float(current_close)}
                    continue
                
                X = X_data[target]
                if len(X) == 0:
                    print(f"   ‚ùå Empty data")
                    predictions[target] = current_close
                    confidence_scores[target] = 50
                    algorithm_predictions[target] = {'ensemble': float(current_close)}
                    continue
                
                # Get latest features
                latest_features = X[-1:].reshape(1,-1)
                
                expected_dim = len(self.feature_columns) * 30
                if latest_features.shape[1] != expected_dim:
                    print(f"   ‚ùå Feature mismatch (got {latest_features.shape[1]}, expected {expected_dim})")
                    predictions[target] = current_close
                    confidence_scores[target] = 50.0
                    algorithm_predictions[target] = {'ensemble': float(current_close)}
                    continue
                
                # Scale features with fallback
                try:
                    latest_scaled = self.feature_scaler.transform(latest_features)
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Feature scaling failed: {e}, using unscaled features")
                    latest_scaled = latest_features
                
                target_predictions = {}
                target_confidences = {}
                target_details = {}
                rejected_predictions = []
                
                print(f"   ü§ñ Individual Model Predictions:")
                print(f"   {'‚îÄ'*40}")
                
                # PREDICT WITH EACH ALGORITHM
                for algo, model in self.models[target].items():
                    if model is None:
                        continue
                    
                    try:
                        pred_actual = None
                        
                        if algo == 'arima':
                            # ARIMA works directly on price series
                            if target in data_with_features.columns:
                                y_recent = data_with_features[target].values[-100:]
                                if len(y_recent) > 30:
                                    try:
                                        temp_arima = pm.auto_arima(
                                            y_recent,
                                            start_p=0, start_q=0,
                                            max_p=2, max_q=2,
                                            seasonal=False,
                                            trace=False,
                                            error_action='ignore',
                                            stepwise=True
                                        )
                                        pred_actual = temp_arima.predict(n_periods=1)[0]
                                        target_details[algo] = {'order': temp_arima.order}
                                    except Exception as e:
                                        pred_actual = data_with_features[target].iloc[-1]
                                else:
                                    pred_actual = data_with_features[target].iloc[-1]
                            else:
                                pred_actual = current_close
                        else:
                            # CRITICAL FIX: Direct prediction, no inverse scaling needed
                            # Models were trained on actual prices, not scaled prices
                            pred_actual = float(model.predict(latest_scaled)[0])
                        
                        # CRITICAL: SANITY CHECK
                        is_valid, confidence_penalty, message = sanity_check_prediction(
                            pred_actual, current_close, algo, max_daily_change=0.15
                        )
                        
                        if not is_valid:
                            rejected_predictions.append((algo, pred_actual, message))
                            print(f"   ‚ùå {algo:20s}: REJECTED - {message}")
                            continue
                        
                        # Calculate base confidence from historical performance
                        confidence = 65  # Reasonable baseline
                        if (target in self.historical_performance and 
                            algo in self.historical_performance[target]):
                            perf = self.historical_performance[target][algo]
                            # Use the pre-calculated confidence
                            confidence = perf.get('confidence', 65)
                        
                        # Apply penalty from sanity check
                        confidence = max(confidence - confidence_penalty, 10)
                        
                        target_predictions[algo] = float(pred_actual)
                        target_confidences[algo] = float(confidence)
                        
                        # PRINT INDIVIDUAL PREDICTION
                        change = ((pred_actual - current_close) / current_close) * 100
                        confidence_symbol = "üü¢" if confidence >= 70 else "üü°" if confidence >= 50 else "üî¥"
                        print(f"   {confidence_symbol} {algo:20s}: ${pred_actual:8.2f} ({change:+6.1f}%) [Conf: {confidence:5.1f}%]")
                        
                    except Exception as e:
                        print(f"   ‚ùå {algo:20s}: ERROR - {str(e)[:50]}")
                        rejected_predictions.append((algo, None, f"Error: {str(e)[:50]}"))
                
                # Show rejected predictions
                if rejected_predictions:
                    print(f"   {'‚îÄ'*40}")
                    print(f"   üö® REJECTED PREDICTIONS:")
                    for algo, pred, reason in rejected_predictions:
                        if pred is not None:
                            print(f"     ‚ùå {algo}: ${pred:.2f} - {reason}")
                        else:
                            print(f"     ‚ùå {algo}: {reason}")
                
                if target_predictions:
                    # FILTER OUTLIERS
                    pred_values = list(target_predictions.values())
                    if len(pred_values) >= 3:
                        median_pred = np.median(pred_values)
                        mad = np.median(np.abs(pred_values - median_pred))
                        
                        filtered_predictions = {}
                        filtered_confidences = {}
                        
                        for algo, pred in target_predictions.items():
                            if mad == 0 or abs(pred - median_pred) <= 3 * mad:
                                filtered_predictions[algo] = pred
                                filtered_confidences[algo] = target_confidences[algo]
                            else:
                                print(f"   ‚ö†Ô∏è Filtered outlier: {algo} (${pred:.2f})")
                        
                        if not filtered_predictions:
                            filtered_predictions = {'median': median_pred}
                            filtered_confidences = {'median': 50.0}
                        
                        target_predictions = filtered_predictions
                        target_confidences = filtered_confidences
                    
                    # Weighted ensemble
                    weights = {}
                    total_weight = 0
                    
                    for algo, conf in target_confidences.items():
                        algo_weight = self.algorithm_weights.get(target, {}).get(algo, 1.0)
                        weight = max(conf / 100, 0.1) * algo_weight
                        weights[algo] = weight
                        total_weight += weight
                    
                    if total_weight > 0:
                        ensemble_pred = sum(
                            pred * (weights[algo] / total_weight)
                            for algo, pred in target_predictions.items()
                        )
                    else:
                        ensemble_pred = np.median(list(target_predictions.values()))
                    
                    # Final sanity check on ensemble
                    ensemble_is_valid, ensemble_penalty, ensemble_msg = sanity_check_prediction(
                        ensemble_pred, current_close, "ENSEMBLE", max_daily_change=0.15
                    )
                    
                    if not ensemble_is_valid:
                        print(f"   üö® ENSEMBLE REJECTED: {ensemble_msg}")
                        ensemble_pred = np.median(list(target_predictions.values()))
                    
                    predictions[target] = float(ensemble_pred)
                    confidence_scores[target] = float(np.median(list(target_confidences.values())))
                    algorithm_predictions[target] = {
                        'individual': target_predictions,
                        'confidences': target_confidences,
                        'weights': weights,
                        'details': target_details,
                        'ensemble': float(ensemble_pred),
                        'ensemble_confidence': float(np.median(list(target_confidences.values())))
                    }
                    
                    # PRINT ENSEMBLE RESULT
                    ensemble_change = ((ensemble_pred - current_close) / current_close) * 100
                    print(f"   {'‚îÄ'*40}")
                    print(f"   üéØ ENSEMBLE {target}: ${ensemble_pred:8.2f} ({ensemble_change:+6.1f}%)")
                    print(f"   üìä Confidence: {confidence_scores[target]:.1f}%")
                    print(f"   üî¢ Models used: {len(target_predictions)}/{len(self.models[target])}")
                else:
                    predictions[target] = float(current_close)
                    confidence_scores[target] = 30.0  # Lower confidence for fallback
                    algorithm_predictions[target] = {'ensemble': float(current_close)}
                    print(f"   ‚ö†Ô∏è Fallback: ${current_close:.2f} (NO VALID MODELS)")
            
            # Ensure we have predictions for all targets
            for target in self.targets:
                if target not in predictions:
                    predictions[target] = float(current_close)
                    confidence_scores[target] = 30.0  # Lower confidence
            
            # Enforce OHLC constraints
            pred_open = predictions.get("Open", current_close)
            pred_close = predictions.get("Close", current_close)
            pred_high = predictions.get("High", current_close * 1.01)
            pred_low = predictions.get("Low", current_close * 0.99)

            pred_high = max(pred_high, pred_open, pred_close)
            pred_low = min(pred_low, pred_open, pred_close)
            
            # Bound predictions to realistic ranges
            max_change = 0.10
            for target in predictions:
                predictions[target] = max(predictions[target], current_close * (1 - max_change))
                predictions[target] = min(predictions[target], current_close * (1 + max_change))

            predictions["High"] = pred_high
            predictions["Low"] = pred_low

            # Generate risk alerts
            risk_alerts = self.generate_risk_alerts(data_with_features, predictions)
            
            # Add split warning if detected
            if has_split:
                risk_alerts.insert(0, {
                    'level': 'üî¥ CRITICAL',
                    'type': 'Stock Split Detected',
                    'message': f'{symbol} has stock splits in historical data',
                    'details': 'Predictions based on post-split data only'
                })
            
            # Calculate prediction confidence metrics
            confidence_metrics = self.calculate_prediction_confidence(predictions, confidence_scores, current_close)
            
            result = {
                'predictions': predictions,
                'algorithm_details': algorithm_predictions,
                'confidence_scores': confidence_scores,
                'confidence_metrics': confidence_metrics,
                'risk_alerts': risk_alerts,
                'current_prices': {
                    'open': float(data['Open'].iloc[-1]) if 'Open' in data.columns else current_close * 0.995,
                    'high': float(data['High'].iloc[-1]) if 'High' in data.columns else current_close * 1.015,
                    'low': float(data['Low'].iloc[-1]) if 'Low' in data.columns else current_close * 0.985,
                    'close': float(current_close)
                },
                'split_info': split_info if has_split else None
            }
            
            # PRINT FINAL SUMMARY
            print(f"\n{'='*70}")
            print(f"üéØ FINAL OCHL PREDICTIONS FOR {symbol}")
            print(f"{'='*70}")
            for target in self.targets:
                if target in predictions:
                    change = ((predictions[target] - current_close) / current_close) * 100
                    conf = confidence_scores.get(target, 50)
                    conf_symbol = "üü¢" if conf >= 70 else "üü°" if conf >= 50 else "üî¥"
                    print(f"   {conf_symbol} {target:6s}: ${predictions[target]:8.2f} ({change:+6.1f}%) [Conf: {conf:5.1f}%]")
            
            print(f"\n   üìä Overall Confidence: {confidence_metrics.get('overall_confidence', 50):.1f}%")
            print(f"   üìà Models used: {sum(len(details.get('individual', {})) for details in algorithm_predictions.values())}")
            
            # WARN about splits
            if has_split:
                print(f"\n   üö® WARNING: {symbol} has stock splits")
                print(f"   Predictions based on POST-SPLIT data only")
            
            print(f"{'='*70}")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error predicting OCHL: {e}")
            import traceback
            traceback.print_exc()
            return self.get_conservative_fallback(data)
    
    def calculate_prediction_confidence(self, predictions, confidence_scores, current_price):
        try:
            if not predictions or not confidence_scores:
                return {}
            
            overall_confidence = np.median(list(confidence_scores.values())) if confidence_scores else 0
            
            predicted_values = list(predictions.values())
            if len(predicted_values) >= 2:
                median_val = np.median(predicted_values)
                if median_val != 0:
                    mad = np.median(np.abs(predicted_values - median_val))
                    if median_val != 0:
                        consistency = 1 - (mad / abs(median_val))
                        consistency_score = max(0, min(100, consistency * 100))
                    else:
                        consistency_score = 0
                else:
                    consistency_score = 0
            else:
                consistency_score = 0
            
            # REALISTIC CONFIDENCE: Don't overly penalize for moderate changes
            # Stocks can move 5-10% in a day, that's normal
            extreme_prediction_penalty = 0
            for target, pred in predictions.items():
                if current_price > 0:
                    pct_change = abs(pred - current_price) / current_price
                    if pct_change > 0.20:  # >20% change is extreme
                        extreme_prediction_penalty += 30
                    elif pct_change > 0.15:  # >15% change
                        extreme_prediction_penalty += 15
                    elif pct_change > 0.10:  # >10% change is moderate
                        extreme_prediction_penalty += 5
            
            overall_confidence = max(overall_confidence - extreme_prediction_penalty, 10)
            
            if overall_confidence >= 70:
                confidence_level = "HIGH"
                confidence_color = "success"
            elif overall_confidence >= 50:
                confidence_level = "MEDIUM"
                confidence_color = "warning"
            elif overall_confidence >= 30:
                confidence_level = "LOW"
                confidence_color = "danger"
            else:
                confidence_level = "VERY LOW"
                confidence_color = "dark"
            
            return {
                'overall_confidence': float(overall_confidence),
                'consistency_score': float(consistency_score),
                'confidence_level': confidence_level,
                'confidence_color': confidence_color,
                'target_confidence': {
                    target: {
                        'score': float(confidence_scores.get(target, 0)),
                        'level': "HIGH" if confidence_scores.get(target, 0) >= 70 else 
                                 "MEDIUM" if confidence_scores.get(target, 0) >= 50 else 
                                 "LOW" if confidence_scores.get(target, 0) >= 30 else "VERY LOW"
                    }
                    for target in self.targets if target in confidence_scores
                }
            }
        except Exception as e:
            print(f"Error calculating prediction confidence: {e}")
            return {}
    
    def generate_risk_alerts(self, data, predictions):
        alerts = []
        
        try:
            current_close = data['Close'].iloc[-1] if 'Close' in data.columns else 100.0
            predicted_close = predictions.get('Close', current_close)
            
            expected_change = ((predicted_close - current_close) / current_close) * 100 if current_close != 0 else 0
            
            if abs(expected_change) > 15:
                alerts.append({
                    'level': 'üî¥ CRITICAL',
                    'type': 'Extreme Price Movement',
                    'message': f'Expected price change of {expected_change:+.1f}%',
                    'details': 'Consider adjusting position size or setting stop-loss'
                })
            elif abs(expected_change) > 10:
                alerts.append({
                    'level': 'üü° HIGH',
                    'type': 'Large Price Movement',
                    'message': f'Expected price change of {expected_change:+.1f}%',
                    'details': 'Monitor position closely'
                })
            elif abs(expected_change) > 5:
                alerts.append({
                    'level': 'üü¢ MEDIUM',
                    'type': 'Moderate Price Movement',
                    'message': f'Expected price change of {expected_change:+.1f}%',
                    'details': 'Normal market movement expected'
                })
            
            if 'Volatility_5d' in data.columns:
                recent_volatility = data['Volatility_5d'].iloc[-10:].mean()
                if recent_volatility > 0.04:
                    alerts.append({
                        'level': 'üî¥ CRITICAL',
                        'type': 'High Volatility',
                        'message': f'Recent volatility: {recent_volatility:.2%}',
                        'details': 'Market showing very high volatility'
                    })
                elif recent_volatility > 0.03:
                    alerts.append({
                        'level': 'üü° HIGH',
                        'type': 'Elevated Volatility',
                        'message': f'Recent volatility: {recent_volatility:.2%}',
                        'details': 'Market showing increased volatility'
                    })
            
            if 'RSI_14' in data.columns and len(data) > 0:
                current_rsi = data['RSI_14'].iloc[-1]
                if current_rsi > 80:
                    alerts.append({
                        'level': 'üü° HIGH',
                        'type': 'Overbought Condition',
                        'message': f'RSI at {current_rsi:.1f} (Overbought)',
                        'details': 'Consider taking profits or waiting for pullback'
                    })
                elif current_rsi < 20:
                    alerts.append({
                        'level': 'üü¢ MEDIUM',
                        'type': 'Oversold Condition',
                        'message': f'RSI at {current_rsi:.1f} (Oversold)',
                        'details': 'Potential buying opportunity'
                    })
            
            if 'Volume_Ratio' in data.columns and len(data) > 0:
                volume_ratio = data['Volume_Ratio'].iloc[-1]
                if volume_ratio > 3.0:
                    alerts.append({
                        'level': 'üî¥ CRITICAL',
                        'type': 'Extreme Volume',
                        'message': f'Volume {volume_ratio:.1f}x average',
                        'details': 'Extreme volume may indicate major news or event'
                    })
                elif volume_ratio > 2.0:
                    alerts.append({
                        'level': 'üü° HIGH',
                        'type': 'Unusual Volume',
                        'message': f'Volume {volume_ratio:.1f}x average',
                        'details': 'High volume may indicate significant news or event'
                    })
            
            return alerts
            
        except Exception as e:
            print(f"Error generating risk alerts: {e}")
            return []
    
    def detect_market_regime(self, data):
        try:
            if len(data) < 20:
                return "NORMAL"
            
            recent = data.tail(20)
            
            volatility = recent['Volatility_5d'].mean() if 'Volatility_5d' in recent.columns else 0
            volume_ratio = recent['Volume_Ratio'].mean() if 'Volume_Ratio' in recent.columns else 1
            rsi = recent['RSI_14'].iloc[-1] if 'RSI_14' in recent.columns else 50
            
            if volatility > 0.03:
                return "HIGH_VOLATILITY"
            elif volume_ratio > 1.5:
                return "HIGH_VOLUME"
            elif rsi > 75 or rsi < 25:
                return "EXTREME_SENTIMENT"
            else:
                return "NORMAL"
                
        except:
            return "NORMAL"

    def check_model_health(self, symbol):
        """Check if models are producing reasonable predictions"""
        health_report = {
            'symbol': symbol,
            'models_available': {},
            'issues': []
        }
        
        for target in self.targets:
            if target not in self.models:
                continue
                
            for algo, model in self.models[target].items():
                if model is None:
                    health_report['models_available'][f"{target}_{algo}"] = False
                    health_report['issues'].append(f"{target}_{algo}: Model is None")
                else:
                    health_report['models_available'][f"{target}_{algo}"] = True
        
        return health_report

# Global predictor instance
predictor = OCHLPredictor()

# ---------------- Helper Functions ----------------
def get_next_trading_day():
    today = datetime.now()
    if today.weekday() == 4:
        return (today + timedelta(days=3)).strftime('%Y-%m-%d')
    if today.weekday() == 5:
        return (today + timedelta(days=2)).strftime('%Y-%m-%d')
    if today.weekday() == 6:
        return (today + timedelta(days=1)).strftime('%Y-%m-%d')
    return (today + timedelta(days=1)).strftime('%Y-%m-%d')

def get_last_market_date():
    today = datetime.now()
    if today.weekday() == 0:
        return (today - timedelta(days=3)).strftime('%Y-%m-%d')
    if today.weekday() == 6:
        return (today - timedelta(days=2)).strftime('%Y-%m-%d')
    if today.weekday() == 5:
        return (today - timedelta(days=1)).strftime('%Y-%m-%d')
    return (today - timedelta(days=1)).strftime('%Y-%m-%d')

def get_market_status():
    today = datetime.now()
    if today.weekday() >= 5:
        return "closed", "Market is closed on weekends"
    
    current_time = datetime.now().time()
    market_open = datetime.strptime('09:30', '%H:%M').time()
    market_close = datetime.strptime('16:00', '%H:%M').time()
    
    if current_time < market_open:
        return "pre_market", "Pre-market hours"
    if current_time > market_close:
        return "after_hours", "After-hours trading"
    
    return "open", "Market is open"

def get_risk_level_from_metrics(risk_metrics):
    if not risk_metrics:
        return "üü¢ LOW RISK"
    
    risk_score = 0
    
    if 'volatility' in risk_metrics:
        risk_score += min(risk_metrics['volatility'] / 50, 1) * 0.3
    
    if 'max_drawdown' in risk_metrics:
        risk_score += min(abs(risk_metrics['max_drawdown']) / 30, 1) * 0.3
    
    if 'var_95' in risk_metrics:
        risk_score += min(abs(risk_metrics['var_95']) / 10, 1) * 0.2
    
    if 'anomaly_score' in risk_metrics:
        risk_score += (risk_metrics['anomaly_score'] / 100) * 0.2
    
    if risk_score > 0.7:
        return "üî¥ EXTREME RISK"
    elif risk_score > 0.5:
        return "üü° HIGH RISK"
    elif risk_score > 0.3:
        return "üü† MEDIUM RISK"
    else:
        return "üü¢ LOW RISK"

def get_trading_recommendation(predictions, current_prices, confidence):
    if confidence < 40:
        return "üö® LOW CONFIDENCE - WAIT"
    
    expected_change = ((predictions.get('Close', current_prices['close']) - current_prices['close']) / current_prices['close']) * 100
    
    if expected_change > 10:
        return "‚úÖ STRONG BUY"
    elif expected_change > 5:
        return "üìà BUY"
    elif expected_change < -10:
        return "üíº STRONG SELL"
    elif expected_change < -5:
        return "üìâ SELL"
    elif abs(expected_change) < 2:
        return "üîÑ HOLD / SIDEWAYS"
    else:
        return "üîÑ HOLD"

# ---------------- NAVIGATION MAP ----------------
NAVIGATION_MAP = {
    'index':'/','jeet':'/jeet','portfolio':'/portfolio','mystock':'/mystock','deposit':'/deposit',
    'insight':'/insight','prediction':'/prediction','news':'/news','videos':'/videos','superstars':'/Superstars',
    'alerts':'/alerts','help':'/help','profile':'/profile'
}

# ---------------- Dynamic routes for NAVIGATION_MAP ----------------
def _find_template_for_page(page_key):
    candidates = [
        f"{page_key}.html",
        f"{page_key.capitalize()}.html",
        f"{page_key.lower()}.html",
        f"{page_key.title()}.html",
    ]
    for fn in candidates:
        full = os.path.join(current_dir, 'templates', fn)
        if os.path.exists(full):
            return fn
    fallback = 'jeet.html'
    if os.path.exists(os.path.join(current_dir, 'templates', fallback)):
        return fallback
    return 'index.html'

# Register routes dynamically
for page_name, route_path in NAVIGATION_MAP.items():
    def make_view(p=page_name):
        def view():
            template_name = _find_template_for_page(p)
            return render_template(template_name, navigation=NAVIGATION_MAP)
        return view
    try:
        server.add_url_rule(route_path, endpoint=page_name, view_func=make_view(), methods=['GET'])
    except AssertionError:
        try:
            server.url_map._rules = [r for r in server.url_map._rules if r.endpoint != page_name]
            server.add_url_rule(route_path, endpoint=page_name, view_func=make_view(), methods=['GET'])
        except Exception:
            pass

@server.route('/navigate/<page_name>')
def navigate_to_page(page_name):
    return redirect(NAVIGATION_MAP.get(page_name, '/'))

# ---------------- API Endpoints ----------------
@server.route('/api/stocks')
@rate_limiter
def get_stocks_list():
    popular_stocks = [
        {"symbol":"AAPL","name":"Apple Inc.","price":271.01,"change":1.24},
        {"symbol":"MSFT","name":"Microsoft Corp.","price":407.57,"change":-0.85},
        {"symbol":"GOOGL","name":"Alphabet Inc.","price":172.34,"change":2.13},
        {"symbol":"AMZN","name":"Amazon.com Inc.","price":178.22,"change":0.67},
        {"symbol":"TSLA","name":"Tesla Inc.","price":175.79,"change":-3.21},
        {"symbol":"GE","name":"GE Aerospace","price":148.25,"change":0.0},
        {"symbol":"PLMR","name":"Palomar Holdings","price":199.47,"change":0.0},
        {"symbol":"META","name":"Meta Platforms Inc.","price":485.00,"change":0.0},
        {"symbol":"NVDA","name":"NVIDIA Corp.","price":620.00,"change":0.0},
        {"symbol":"NKE","name":"Nike Inc.","price":110.00,"change":0.0}
    ]
    
    for stock in popular_stocks:
        try:
            t = yf.Ticker(stock['symbol'])
            h = t.history(period='1d', interval='1m')
            if not h.empty:
                current = h['Close'].iloc[-1]
                prev_close = t.info.get('previousClose', current)
                change = ((current-prev_close)/prev_close)*100 if prev_close!=0 else 0.0
                stock['price'] = round(current,2)
                stock['change'] = round(change,2)
        except Exception:
            continue
    
    return jsonify(popular_stocks)

@server.route('/api/health')
def health_check():
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "7.0.0",  # MAJOR VERSION UPDATE
        "algorithms": ["Linear Regression", "Ridge", "Lasso", "SVR", "Random Forest", "Gradient Boosting", "ARIMA", "Neural Network"],
        "features": "Complete OCHL Prediction with 90+ Features",
        "data": "10 Years Historical Data",
        "split_handling": "Enabled (GE, AAPL, TSLA, NVDA, GOOGL, AMZN)",
        "sanity_checks": "Enabled (rejects >20% daily changes)",
        "critical_fixes": [
            "REMOVED target price scaling (was causing insane predictions)",
            "REALISTIC confidence calculation (direction accuracy focused)",
            "FIXED unpacking errors in get_reliable_predictions",
            "IMPROVED performance metrics using percentage returns"
        ],
        "realistic_expectations": "55-65% direction accuracy is good for stocks"
    })

@server.route('/api/predict', methods=['POST'])
@rate_limiter
def predict_stock():
    """Main prediction endpoint with detailed output - FIXED VERSION"""
    try:
        data = request.get_json() or {}
        symbol = (data.get('symbol') or 'AAPL').upper().strip()
        
        print(f"\n{'='*70}")
        print(f"üöÄ PREDICTION REQUEST FOR {symbol}")
        print(f"{'='*70}")
        
        # Get historical data - 10 YEARS
        historical_data, current_price, error = get_live_stock_data_enhanced(symbol)
        if error:
            return jsonify({"error": str(error)}), 400
        
        print(f"üìä Data fetched: {len(historical_data)} days")
        print(f"üí∞ Current price: ${current_price:.2f}")
        
        # Check for splits
        clean_data, has_split, split_info = detect_and_handle_splits(historical_data, symbol)
        if has_split:
            print(f"üö® SPLIT HANDLING: Using post-split data only")
            print(f"   Split info: {split_info}")
            current_price = float(clean_data['Close'].iloc[-1]) if len(clean_data) > 0 else current_price
        
        # Load or train models
        models_loaded = predictor.load_models(symbol)
        
        if not models_loaded or not predictor.is_fitted:
            print("üî® Training new models...")
            success, train_msg = predictor.train_all_models(clean_data if has_split else historical_data, symbol)
            if not success:
                return provide_fallback_prediction(symbol, historical_data)
            print("‚úÖ Training successful")
        else:
            print("‚úÖ Loaded existing models")
        
        # Get reliable predictions - FIXED UNPACKING
        print("\nü§ñ Making predictions with enhanced reliability...")
        prediction_result = predictor.get_reliable_predictions(symbol, clean_data if has_split else historical_data)
        
        # Check if this is a fallback prediction
        is_fallback = prediction_result.get('fallback', False)
        
        # Prepare response
        current_prices = prediction_result['current_prices']
        predictions = prediction_result['predictions']
        confidence_scores = prediction_result['confidence_scores']
        confidence_metrics = prediction_result['confidence_metrics']
        risk_alerts = prediction_result['risk_alerts']
        algorithm_details = prediction_result.get('algorithm_details', {})
        split_info_response = prediction_result.get('split_info')
        
        # Calculate expected changes
        expected_changes = {}
        for target in ['Open', 'High', 'Low', 'Close']:
            if target in predictions and target.lower() in current_prices:
                current = current_prices.get(target.lower(), current_price)
                predicted = predictions[target]
                change = ((predicted - current) / current) * 100 if current != 0 else 0
                expected_changes[target] = change
        
        # Generate trading recommendation
        overall_confidence = confidence_metrics.get('overall_confidence', 70)
        recommendation = get_trading_recommendation(predictions, current_prices, overall_confidence)
        
        # Get risk level
        risk_level = get_risk_level_from_metrics(predictor.risk_metrics)
        
        # Format algorithm performance if available
        algorithm_performance = {}
        if not is_fallback and predictor.historical_performance:
            for target in predictor.historical_performance:
                algorithm_performance[target] = {}
                for algo, perf in predictor.historical_performance[target].items():
                    algorithm_performance[target][algo] = {
                        'direction_accuracy': round(perf.get('direction_accuracy', 0), 1),
                        'confidence': round(perf.get('confidence', 0), 1),
                        'mape': round(perf.get('mape', 0), 1),
                        'weight': round(predictor.algorithm_weights.get(target, {}).get(algo, 0.1), 2)
                    }
        
        response = {
            "symbol": symbol,
            "timestamp": datetime.now().isoformat(),
            "market_status": get_market_status()[1],
            "last_trading_day": get_last_market_date(),
            "prediction_date": get_next_trading_day(),
            
            "current_prices": current_prices,
            
            "predictions": {
                target: {
                    "predicted_price": round(predictions.get(target, current_prices.get(target.lower(), current_price)), 2),
                    "current_price": round(current_prices.get(target.lower(), current_price), 2),
                    "expected_change": round(expected_changes.get(target, 0), 2),
                    "confidence": round(confidence_scores.get(target, 70), 1)
                }
                for target in ['Open', 'High', 'Low', 'Close']
            },
            
            "algorithm_predictions": algorithm_details,
            
            "confidence_metrics": confidence_metrics,
            
            "historical_performance": algorithm_performance,
            
            "risk_metrics": predictor.risk_metrics,
            
            "risk_alerts": risk_alerts,
            
            "risk_level": risk_level,
            "trading_recommendation": recommendation,
            
            "prediction_history": predictor.prediction_history.get(symbol, [])[-10:],
            
            "model_info": {
                "last_training_date": predictor.last_training_date,
                "is_fitted": predictor.is_fitted,
                "targets_trained": list(predictor.models.keys()),
                "feature_count": len(predictor.feature_columns),
                "algorithms_used": ["linear_regression", "ridge", "lasso", "svr", "random_forest", "gradient_boosting", "arima", "neural_network"],
                "algorithm_weights": predictor.algorithm_weights,
                "fallback_mode": is_fallback
            },
            
            "data_info": {
                "total_days": len(historical_data),
                "date_range": {
                    "start": historical_data['Date'].iloc[0] if 'Date' in historical_data.columns else "N/A",
                    "end": historical_data['Date'].iloc[-1] if 'Date' in historical_data.columns else "N/A"
                },
                "split_info": split_info_response,
                "post_split_data_used": has_split
            },
            
            "insight": f"AI predicts {expected_changes.get('Close', 0):+.1f}% change for {symbol}. {recommendation}. Confidence: {overall_confidence:.1f}%"
        }
        
        if is_fallback:
            response["fallback_warning"] = "Using enhanced conservative predictions due to limited model availability"
        
        print(f"\n{'='*70}")
        print(f"üéØ FINAL RESPONSE READY")
        if is_fallback:
            print(f"‚ö†Ô∏è USING ENHANCED FALLBACK PREDICTIONS")
        print(f"{'='*70}")
        
        return jsonify(response)
        
    except Exception as e:
        print(f"‚ùå Prediction endpoint error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({
            "error": "Prediction service temporarily unavailable",
            "fallback": True,
            "symbol": symbol if 'symbol' in locals() else 'UNKNOWN'
        }), 500

@server.route('/api/train', methods=['POST'])
@rate_limiter
def train_models():
    try:
        data = request.get_json() or {}
        symbol = (data.get('symbol') or 'AAPL').upper().strip()
        
        print(f"\nüî® TRAINING REQUEST FOR {symbol}")
        
        historical_data, current_price, error = get_live_stock_data_enhanced(symbol)
        if error:
            return jsonify({"error": str(error)}), 400
        
        success, train_msg = predictor.train_all_models(historical_data, symbol)
        
        if success:
            # Check model health after training
            health_report = predictor.check_model_health(symbol)
            
            return jsonify({
                "status": "success",
                "message": train_msg,
                "symbol": symbol,
                "last_training_date": predictor.last_training_date,
                "historical_performance": predictor.historical_performance,
                "risk_metrics": predictor.risk_metrics,
                "algorithm_weights": predictor.algorithm_weights,
                "model_health": health_report
            })
        else:
            return jsonify({
                "status": "error",
                "message": train_msg,
                "symbol": symbol
            }), 400
            
    except Exception as e:
        print(f"Training endpoint error: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@server.route('/api/history/<symbol>')
@rate_limiter
def get_prediction_history(symbol):
    try:
        symbol = symbol.upper()
        
        history_path = os.path.join(HISTORY_DIR, f"{symbol}_history.json")
        if os.path.exists(history_path):
            with open(history_path, 'r') as f:
                history_data = json.load(f)
            
            return jsonify({
                "status": "success",
                "symbol": symbol,
                "history": history_data.get('prediction_history', []),
                "performance": history_data.get('historical_performance', {}),
                "algorithm_weights": history_data.get('algorithm_weights', {}),
                "last_updated": history_data.get('last_training_date')
            })
        else:
            return jsonify({
                "status": "error",
                "message": "No history found for this symbol"
            }), 404
            
    except Exception as e:
        print(f"History endpoint error: {e}")
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

def provide_fallback_prediction(symbol, historical_data):
    try:
        print(f"‚ö†Ô∏è Using ultimate fallback prediction for {symbol}")
        
        if historical_data is None or historical_data.empty:
            current_price = 100.0
        else:
            current_price = historical_data['Close'].iloc[-1] if 'Close' in historical_data.columns else 100.0
        
        # Very conservative fallback based on simple moving average
        predictions = {}
        for target in ['Open', 'High', 'Low', 'Close']:
            # Simple trend following
            if 'Close' in historical_data.columns and len(historical_data) > 5:
                recent_avg = historical_data['Close'].iloc[-5:].mean()
                trend = 1.001 if current_price > recent_avg else 0.999
            else:
                trend = 1.0
            
            if target == 'High':
                pred = current_price * trend * 1.005  # Slight upward bias for high
            elif target == 'Low':
                pred = current_price * trend * 0.995  # Slight downward bias for low
            elif target == 'Open':
                pred = current_price * trend
            else:  # Close
                pred = current_price * trend
            
            predictions[target] = pred
        
        # Ensure OHLC constraints
        pred_open = predictions.get("Open", current_price)
        pred_close = predictions.get("Close", current_price)
        pred_high = max(predictions.get("High", current_price * 1.01), pred_open, pred_close)
        pred_low = min(predictions.get("Low", current_price * 0.99), pred_open, pred_close)
        
        predictions["High"] = pred_high
        predictions["Low"] = pred_low
        
        changes = {}
        for target, pred in predictions.items():
            change = ((pred - current_price) / current_price) * 100 if current_price != 0 else 0
            changes[target] = change
        
        return jsonify({
            "symbol": symbol,
            "current_prices": {
                "open": round(current_price * 0.995, 2),
                "high": round(current_price * 1.015, 2),
                "low": round(current_price * 0.985, 2),
                "close": round(current_price, 2)
            },
            "predictions": {
                target: {
                    "predicted_price": round(pred, 2),
                    "expected_change": round(changes[target], 2),
                    "confidence": 60
                }
                for target, pred in predictions.items()
            },
            "confidence_metrics": {
                "overall_confidence": 60,
                "confidence_level": "MEDIUM",
                "confidence_color": "warning"
            },
            "risk_alerts": [{
                "level": "üü° HIGH",
                "type": "Ultimate Fallback Mode",
                "message": "Using ultimate fallback prediction engine",
                "details": "Primary models and enhanced fallbacks unavailable"
            }],
            "risk_level": "üü† MEDIUM RISK",
            "trading_recommendation": "üîÑ HOLD",
            "fallback": True,
            "message": "Using ultimate fallback prediction engine"
        })
        
    except Exception as e:
        print(f"Ultimate fallback prediction error: {e}")
        return jsonify({
            "error": "Prediction service unavailable",
            "fallback": True
        }), 500

# ---------------- Serve static/templates ----------------
@server.route('/static/<path:path>')
def serve_static(path):
    return send_from_directory(os.path.join(current_dir,'static'), path)

# ---------------- Error handlers ----------------
@server.errorhandler(404)
def not_found(error): 
    return jsonify({"error":"Page not found"}), 404

@server.errorhandler(500)
def internal_error(error): 
    return jsonify({"error":"Internal server error"}), 500

# ---------------- Run ----------------
if __name__ == '__main__':
    print("=" * 70)
    print("üìà STOCK MARKET PREDICTION SYSTEM v7.0.0")
    print("=" * 70)
    print("‚ú® CRITICAL FIXES APPLIED:")
    print("  ‚Ä¢ üö® REMOVED target price scaling (was causing insane predictions)")
    print("  ‚Ä¢ ‚úÖ Models now predict ACTUAL prices, not scaled values")
    print("  ‚Ä¢ üìä REALISTIC confidence based on direction accuracy (55-65% is good)")
    print("  ‚Ä¢ üîß FIXED unpacking errors in get_reliable_predictions")
    print("  ‚Ä¢ üõ°Ô∏è More realistic sanity checks (stocks can move 5-15% normally)")
    print("=" * 70)
    print("üîß TECHNICAL:")
    print("  ‚Ä¢ 10 YEARS historical data")
    print("  ‚Ä¢ 8 ALGORITHMS trained on actual prices")
    print("  ‚Ä¢ 90+ TECHNICAL FEATURES")
    print("  ‚Ä¢ REALISTIC performance expectations")
    print("=" * 70)
    print("üöÄ Ready for predictions! Send POST to /api/predict")
    print("=" * 70)
    
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    os.makedirs(MODELS_DIR, exist_ok=True)
    os.makedirs(HISTORY_DIR, exist_ok=True)
    
    port = int(os.environ.get('PORT', 8080))
    server.run(host='0.0.0.0', port=port, debug=True, threaded=True)